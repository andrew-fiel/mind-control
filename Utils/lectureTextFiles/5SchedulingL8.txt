CS370 Operating Systems
Colorado State University
Yashwant K Malaiya
Spring 2020 Lecture 8 
Scheduling
Slides based on 
• Text by Silberschatz, Galvin, Gagne
• Various sources
1 1
FAQ
• A process is isolated from other processes. Processes can run concurrently.
• A thread is not isolated from other threads belonging to the same process. Runs 
concurrently with other threads.  
• POSIX: Portable Operating System Interface is a family of IEEE standards. It defines application 
programming interface (API), command line shells and utility interfaces, compatibility with variants of OSs.
• Processes/threads/IPC/IO.
• What is a pthread? POSIX compliant implementation of threads.
• A function when called within a new thread, runs concurrently with other threads.
• Java threads: user threads or kernel threads? Most 
JVMs implement threads with native, OS level threads,
• Examples of threads: Self exercise set 4
2
FAQ
• Why use threads: 
– Parallelism if multiple cores/hyper-threading available.
– Concurrency: quicker responses to some of the things like refreshing output, checking 
spelling as one types etc.
• Implicit threading: thread creating automated: compiler assisted higher 
level programming
• Unix signals vs interrupts:  Signals are a limited form of inter-process 
communication. Interrupts are often initiated by hardware. In both cases, some specific routines 
respond.
• Hyper-threading: Requires additional hardware. Widely used
• Signals example (assume pid = 162):  kill -9  162     or   kill –s  sigkill 162
• Pthread example:    pthread_kill(ThreadID, SIGKILL );
3
Chapter 5:  CPU Scheduling
• Basic Concepts
• Scheduling Criteria 
• Scheduling Algorithms
• Thread Scheduling
• Multiple-Processor Scheduling
• Real-Time CPU Scheduling
• Operating Systems Examples
• Algorithm Evaluation
4
Diagram of Process State
Ready to Running: scheduled by scheduler
Running to Ready: scheduler picks another process, back in ready queue
Running to Waiting (Blocked) : process blocks for input/output
Waiting to Ready: Input available
5
Basic Concepts
• Maximum CPU 
utilization obtained 
with multiprogramming
• CPU–I/O Burst Cycle –
Process execution 
consists of a cycle of 
CPU execution and I/O 
wait
• CPU burst followed by 
I/O burst
• CPU burst distribution 
is of main concern
6
Histogram of CPU-burst Times
Typical distribution of CPU bursts. Most CPU bursts are just a few ms.
7
CPU Scheduler
 Short-term scheduler selects from among the processes 
in ready queue, and allocates the CPU to one of them
 Queue may be ordered in various ways
 CPU scheduling decisions may take place when a process:
1. Switches from running to waiting state
2. Switches from running to ready state Not
Controlled by 
3. Switches from waiting to ready the  process
4. Terminates
 Scheduling under 1 and 4 is nonpreemptive
 All other scheduling is preemptive. These need to be 
considered
 access to shared data by multiple processes
 preemption while in kernel mode
 interrupts occurring during crucial OS activities
8
Dispatcher
• Dispatcher module gives control of the 
CPU to the process selected by the short-
term scheduler; this involves:
– switching context
– switching to user mode
– jumping to the proper location in the user 
program to restart that program
• Dispatch latency – time it takes for the 
dispatcher to stop one process and start 
another running
9
The Dispatcher (dentist’s office)
10
Scheduling Criteria
• CPU utilization – keep the CPU as busy as 
possible: Maximize
• Throughput – # of processes that complete their 
execution per time unit: Maximize
• Turnaround time –time to execute a process 
from submission to completion:  Minimize
• Waiting time – amount of time a process has 
been waiting in the ready queue: Minimize
• Response time –time it takes from when a 
request was submitted until the first response is 
produced, not output  (for time-sharing 
environment): Minimize
11
Terms for a single process
UCLA
12
Scheduling Algorithms
We will now examine several major scheduling 
approaches
• Decide which process in the ready queue is 
allocated the CPU 
• Could be preemptive or nonpreemptive
– preemptive: remove in middle of execution 
(“forced”) Involuntary 
deboarding! 
• Optimize measure of interest 
– We will use Gantt charts to illustrate schedules 
– Bar chart with start and finish times for processes 
13
Nonpreemptive vs   Preemptive  sheduling
• Nonpreemptive: Process keeps CPU until it 
relinquishes it when
– It terminates
– It switches to the waiting state
– Used by initial versions of OSs like Windows 3.x
• Preemptive scheduling
– Pick a process and let it run for a maximum of some 
fixed time
– If it is still running at the end of time interval?
• Suspend it  and  pick another process to run
• A clock interrupt at the end of the time interval 
to  give control back of CPU back to scheduler
14
Scheduling Algorithms
• First- Come, First-Served (FCFS) 
• Shortest-Job-First (SJF) 
– Shortest-remaining-time-first
• Priority Scheduling
• Round Robin (RR) with time quantum
• Multilevel Queue
– Multilevel Feedback Queue
• “Completely fair”
Comparing Performance
• Average waiting time etc.
15
First- Come, First-Served (FCFS) Scheduling
• Process requesting CPU first, gets it first 
• Managed with a FIFO queue
– When process enters ready queue 
• PCB is tacked to the tail of the queue 
– When CPU is free
• It is allocated to process at the head of the queue 
• Simple to write and understand 
16
First- Come, First-Served (FCFS) Scheduling
Process Burst Time
Henry Gantt, P1 24
1910s P2 3
P3 3
• Suppose that the processes arrive in the order: P1 , P2 , 
P3  but almost the same time.
The Gantt Chart for the schedule is:
P P P
1 2 3
0 24 27 30
• Waiting time for P1 =   ;  P2 =   ; P3 =     
• Average waiting time:  (      +      +     )/  =  
• Throughput:         /          = per unit time
Pause for students to do the computation
17
First- Come, First-Served (FCFS) Scheduling
Process Burst Time
Henry Gantt, P1 24
1910s P2 3
P3 3
• Suppose that the processes arrive in the order: P1 , P2 , 
P3  but almost the same time.
The Gantt Chart for the schedule is:
P P P
1 2 3
0 24 27 30
• Waiting time for P1 =  0;  P2 =  24; P3 =  27
• Average waiting time:  (   0  +  24   + 27 )/3 = 17
• Throughput:        3/ 30 = 0.1 per unit time
18
FCFS Scheduling (Cont.)
Suppose that the processes arrive in the order:
P2 , P3 , P1
• The Gantt chart for the schedule is:
P P P
2 3 1
0 3 6 30
• Waiting time for P1 = 6; P2 = 0; P3 = 3
• Average waiting time:   (6 + 0 + 3)/3 = 3
– Much better than previous case
• But note -Throughput: 3/30 = 0.1 per unit same
• Convoy effect - short process behind long process
– Consider one CPU-bound and many I/O-bound processes
19
Shortest-Job-First (SJF) Scheduling
• Associate with each process the length of its next 
CPU burst
– Use these lengths to schedule the process with the 
shortest time
• Reduction in waiting time for short process 
GREATER THAN  Increase in waiting time for long 
process
• SJF is optimal – gives minimum average waiting 
time for a given set of processes
– The difficulty is knowing the length of the next CPU 
request
– Estimate or could ask the user
20
Example of SJF
ProcessArriva l TimeBurst Time
P1 0.0 6
P2 2.0 8
P3 4.0 7
P4 5.0 3
• All arrive at time 0.
• SJF scheduling chart
• Average waiting time for P1,P2,P3,P4 = (      +       +      +      ) /   =
Pause for students to do the computation
21
Example of SJF
ProcessArriva l TimeBurst Time
P1 0.0 6
P2 2.0 8
P3 4.0 7
P4 5.0 3
• All arrive at time 0.
• SJF scheduling chart
P P P P
4 1 3 2
0 3 9 16 24
• Average waiting time for P1,P2,P3,P4 = (3 + 16 + 9 + 0) / 4 = 7
22
Determining Length of Next CPU Burst
• Can only estimate the length – should be similar to 
the recent bursts
– Then pick process with shortest predicted next CPU burst
• Can be done by using the length of previous CPU 
bursts, using exponential averaging
1.  tn  actual  length of n
th CPU  burst
2.   n1  predicted value for the next CPU  burst
3.   , 0    1
4.  Define :  n1  tn  1  n.
• Commonly, α set to ½
23
Prediction of the Length of the Next CPU Burst
Blue points: guess
Black points: actual
α = 0.5
Ex:
0.5x6 +0.5x10 = 8
24
Examples of Exponential Averaging
•  =0
Widely used for 
– n+1 = n predicting stock-
– Recent history does not count
market etc
•  =1
– n+1 =  tn
– Only the actual last CPU burst counts
•  n1   tn  1  n.
• If we expand the formula, substituting for  n , we 
get:
n+1 =  tn+(1 - ) tn -1 + …
+(1 -  )j tn -j + …
+(1 -  )n +1 0
• Since both  and (1 - ) are less than or equal to 
1, each successive term has less weight than its 
predecessor
25
Shortest-remaining-time-first (preemptive SJF)
• Preemptive version called shortest-remaining-time-first
• Now we add the concepts of varying arrival times and 
preemption to the analysis
ProcessAarri Arrival TimeT Burst Time
P1 0 8
P2 1 4 (will preempt because 4<7)
P3 2 9 (will not preempt)
P4 3 5
• Preemptive SJF Gantt Chart  
P P P P P
1 2 4 1 3
0 1 5 10 17 26
• Average waiting time for P1,P2,P3,P4
= [(10-1)+(1-1)+(17-2)+(5-3)]/4 = 26/4 = 6.5 msec
26
Priority Scheduling
• A priority number (integer) is associated with each 
process
• The CPU is allocated to the process with the highest 
priority (smallest integer  highest priority)
– Preemptive
– Nonpreemptive
• SJF is priority scheduling where priority is the inverse of 
predicted next CPU burst time
• Problem  Starvation – low priority processes may 
never execute
– Solution  Aging – as time progresses increase the priority of 
the process
MIT had a low priority job waiting from 1967 to 1973 on IBM 7094!  
27
Example of Priority Scheduling
ProcessA arri Burst TimeT Priority
P1 10 3
P2 1 1 (highest)
P3 2 4
P4 1 5
P5 5 2
• P1,P2, P3, P4,P5  all arrive at time 0. 
• Priority scheduling Gantt Chart
• Average waiting time  for P1, .. P5: (6+0+16+18+1)/5 = 8.2 msec
28
Round Robin (RR) with time quantum
• Each process gets a small unit of CPU time (time quantum
q), usually 10-100 milliseconds.  After this, the process is 
preempted, added to the end of the ready queue.
• If there are n processes in the ready queue and the time 
quantum is q, then each process gets 1/n of the CPU time 
in chunks of at most q time units at once.  No process 
waits more than (n-1)q time units.
• Timer interrupts every quantum to schedule next process
• Performance
– q large  FIFO
– q small  q must be large with respect to context switch, 
otherwise overhead is too high  (overhead typically in 0.5% 
range)
29
Example of RR with Time Quantum = 4
Process Burst Time
P1 24
P2 3
P3 3
• Arrive a time 0 in order P1, P2, P3: The Gantt chart is: 
P P P P P P P P
1 2 3 1 1 1 1 1
0 4 7 10 14 18 22 26 30
• Waiting times: P1:10-4 =6, P2:4, P3:7, average 17/3 = 5.66 
units
• Typically, higher average turnaround than SJF, but better 
response
• q should be large compared to context switch time
• q usually 10ms to 100ms, context switch < 10 µsec
Response time: Arrival to beginning of execution
Turnaround time: Arrival to finish of execution
30
Time Quantum and Context Switch Time
Much smaller quantum compared to burst: many switches
31
Turnaround Time Varies With The Time Quantum
Rule of thumb: 80% of CPU bursts 
should be shorter than q
Illustration
q=7:
Turnaround times for P1,P2,P3,P4: 
6,9,10,17  av = 10.5
Similarly for q =1, ..6 (verify yourself)
Students: Repeat for q = 1, ..6 at home to verify the plot.
32
Multilevel Queue
• Ready queue is partitioned into separate queues, 
e.g.:
– foreground (interactive)
– background (batch)
• Process permanently in a given queue
• Each queue has its own scheduling algorithm, e.g.:
– foreground – RR
– background – FCFS
• Scheduling must be done between the queues:
– Fixed priority scheduling; (i.e., serve all from foreground 
then from background).  Possibility of starvation.  Or
– Time slice – each queue gets a certain amount of CPU 
time which it can schedule amongst its processes; i.e.,     
80% to foreground in RR, 20% to background in FCFS 
33
Multilevel Queue Scheduling
Real-time processes may have the highest priority.
34
Multilevel Feedback Queue
• A process can move between the various queues; 
aging can be implemented this way
• Multilevel-feedback-queue scheduler defined by 
the following parameters:
– number of queues
– scheduling algorithms for each queue
– method used to determine when to upgrade a process
– method used to determine when to demote a process
– method used to determine which queue a process will 
enter when that process needs service
Inventor Corbato won the Touring award!
35
Example of Multilevel Feedback Queue
• Three queues: 
– Q0 – RR with time quantum 8 milliseconds
– Q1 – RR time quantum 16 milliseconds
– Q2 – FCFS (no time quantum limit)
• Scheduling
– A new job enters queue Q0 which is served
FCFS
• When it gains CPU, job receives 8 
milliseconds
• If it does not finish in 8 milliseconds, 
job is moved to queue Q1
– At Q1 job is again served FCFS and receives 
16 additional milliseconds
• If it still does not complete, it is 
preempted and moved to queue Q2
Upgrading may be based on aging. Periodically processes may be moved to the top level.
Variations of the scheme were used in earlier versions of Linux.  
36
Thread Scheduling
• Thread scheduling is similar
• Distinction between user-level and kernel-level threads
• When threads supported, threads scheduled, not processes
Scheduling competition
• Many-to-one and many-to-many models, thread library schedules 
user-level threads to run on LWP
– Known as process-contention scope (PCS) since scheduling competition is 
within the process
– Typically done via priority set by programmer
• Kernel thread scheduled onto available CPU is system-contention 
scope (SCS) – competition among all threads in system
• Pthread API allows both, but Linux and Mac OSX allows only SCS.
LWP layer between kernel threads and user threads in some older OSs
37
Multiple-Processor Scheduling
• CPU scheduling more complex when multiple CPUs are 
available. 
• Assume Homogeneous processors within a 
multiprocessor
• Asymmetric multiprocessing – individual processors can 
be dedicated to specific tasks at design time
• Symmetric multiprocessing (SMP) – each processor is 
self-scheduling, 
– all processes in common ready queue, or
– each has its own private queue of ready processes
• Currently, most common
• Processor affinity – process has affinity for processor on 
which it is currently running because of info in cache
– soft affinity: try but no guarantee
– hard affinity  can specify processor sets
38
NUMA and CPU Scheduling
Note that memory-placement algorithms can also consider affinity
Non-uniform memory access (NUMA), in which a CPU has 
faster access to some parts of main memory. 
39
Multiple-Processor Scheduling – Load Balancing
• If SMP, need to keep all CPUs loaded for 
efficiency
• Load balancing attempts to keep workload 
evenly distributed 
– Push migration – periodic task checks load on 
each processor, and if found pushes task from 
overloaded CPU to other CPUs
– Pull migration – idle processors pulls waiting 
task from busy processor
– Combination of push/pull may be used.
40
Multicore Processors
• Recent trend to place multiple processor 
cores on same physical chip
• Faster and consumes less power
• Multiple threads per core
– Concurrent
– Parallel: with hyper-threading hardware
41
Real-Time CPU Scheduling
• Can present obvious challenges
– Soft real-time systems – no guarantee as to when critical 
real-time process will be scheduled
– Hard real-time systems – task must be serviced by its 
deadline
• For real-time scheduling, scheduler must support 
preemptive, priority-based scheduling
– But only guarantees soft real-time
• For hard real-time must also provide ability to 
meet deadlines
– periodic ones require CPU at constant intervals
43
Virtualization and Scheduling
• Virtualization software schedules multiple 
guests onto CPU(s)
• Each guest doing its own scheduling
– Not knowing it doesn’t own the CPUs
– Can effect time-of-day clocks in guests
• VMM has its own scheduler
• Various approaches have been used
– Workload aware, Guest OS cooperation, etc.
44
Operating System Examples
• Solaris scheduling: 6 classes, Inverse relationship 
between priorities and time quantum
• Windows XP scheduling: 32 priority levels (real-time, 
not real-time levels)
• Linux scheduling schemes have continued to evolve.
• Linux Completely fair scheduler (CFS, 2007): 
– Variable time-slice based on number and priority of the tasks 
in the queue.
– Maximum execution time based on waiting processes (Q/n). 
– Processes kept in a red-black binary tree with scheduling 
complexity of O(log N)
– Process with lowest weighted spent execution (virtual run 
time) time is picked next. Weighted by priority (“niceness”). 
45

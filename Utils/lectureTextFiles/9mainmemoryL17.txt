CS370 Operating Systems
Colorado State University
Yashwant K Malaiya
Spring 2020 L17
Main Memory
Slides based on 
• Text by Silberschatz, Galvin, Gagne
• Various sources
1 1
Welcome to CS370 Second Half
• Topics: Memory, Storage, File System etc
• Class rules: See Syllabus
– Class, Canvas, Piazza
– Handheld devices, Respect
– participation 
• now virtual iClicker questions in  ICx (due wed) and Qx (due 
Mon)
– On-line quizzes, Final - Section
– Project, deadlines, Plagiarism
2
FAQ
• Deadlock avoidance
– Resource allocation graph
– Bankers algorithm: Max/Allocation
• Deadlock detection
– Wait-for-graph
– General Algorithm: Allocation/Request (similar to Banker’s algorithm)
• Can CPU execute program directly from the disk?
• Can the disk be replaced by semiconductor memory?
3
Hierarchy
Main memory and registers are only 
storage CPU can access directly 
access
Register access in one CPU clock (or 
Registers less).Main memory can take many cycles, 
causing a stall.
Ch 9 Cache sits between main memory Cache and CPU registers making main memory appear much faster 
Main Memory
Ch 
10 Secondary Memory (Disk) Removable/Backup
Ch 11, 13,14,15: Disk, file system      Cache Memory: CS470   
4
Logical vs. Physical Address Space
• The concept of a logical address space that 
is bound to a separate physical address 
space is central to proper memory 
management
– Logical address – generated by the CPU; also 
referred to as virtual address
– Physical address – address seen by the 
memory unit
• Logical address space is the set of all 
logical addresses generated by a program
• Physical address space is the set of all 
physical addresses
5
Memory Allocation Approaches
6
Memory Allocation Approaches
• Contiguous allocation: entire memory for 
a program in a single contiguous memory 
block. Find where a program will “fit”. earliest 
approach
• Segmentation: program divided into 
logically divided “segments” such as main 
program, function, stack etc. 
– Need table to track segments.
• Paging: program divided into fixed size 
“pages”, each placed in a fixed size 
“frame”. 
– Need table to track pages.
7
Contiguous Allocation
• Main memory must support both OS and 
user processes
• Limited resource, must allocate efficiently
• Contiguous allocation is one early method
• Main memory usually into two partitions:
– Resident operating system, usually held in low 
memory with interrupt vectors
– User processes then held in high memory
– Each process contained in single contiguous 
section of memory
8
Contiguous Allocation (Cont.)
• Registers used to protect user processes 
from each other, and from changing 
operating-system code and data
– Relocation (Base) register contains value of 
smallest physical address
– Limit register contains range of logical 
addresses – each logical address must be less 
than the limit register 
• MMU maps logical address dynamically
9
Hardware Support for Relocation and Limit Registers
MMU maps logical address dynamically
Physical address = relocation reg + valid logical address
10
Multiple-partition allocation
• Multiple-partition allocation
– Degree of multiprogramming limited by number of partitions
– Variable-partition sizes for efficiency (sized to a given process’ needs)
– Hole – block of available memory; holes of various size are scattered 
throughout memory
– When a process arrives, it is allocated memory from a hole large enough to 
accommodate it
– Process exiting frees its partition, adjacent free partitions combined
– Operating system maintains information about:
a) allocated partitions    b) free partitions (hole)
11
Dynamic Storage-Allocation Problem
How to satisfy a request of size n from a list of free holes?
• First-fit:  Allocate the first hole that is big enough
• Best-fit:  Allocate the smallest hole that is big enough; must 
search entire list, unless ordered by size  
– Produces the smallest leftover hole
• Worst-fit:  Allocate the largest hole; must also search entire 
list  
– Produces the largest leftover hole
Simulation studies:
• First-fit and best-fit better than worst-fit in terms of speed and storage 
utilization
• Best fit is slower than first fit .  Surprisingly, it also results in more 
wasted memory than first fit
• Tends to fill up memory with tiny, useless holes 
12
Fragmentation
• External Fragmentation – External fragmentation:
memory wasted due to small chunks of free memory
interspersed among allocated regions
• Internal Fragmentation – allocated memory may be
slightly larger than requested memory; this size
difference is memory internal to a partition, but not
being used
• Simulation analysis reveals that given N blocks
allocated, 0.5 N blocks lost to fragmentation
– 1/3 may be unusable -> 50-percent rule
13
Fragmentation (Cont.)
• Reduce external fragmentation by compaction
– Shuffle memory contents to place all free memory 
together in one large block
– Compaction is possible only if relocation is dynamic, 
and is done at execution time
– I/O problem
• Latch job in memory while it is involved in I/O
• Do I/O only into OS buffers
14
Paging vs Segmentations
Segmentation: program divided into logically divided 
“segments” such as main program, function, stack etc. 
• Need table to track segments.
• Term “segmentation fault occurs”: improper 
attempt to access a memory location
Paging: program divided into fixed size “pages”, each 
placed in a fixed size “frame”. 
• Need table to track pages.
• No external fragmentation
• Increasingly more common
17
Paging vs Segmentations
18
Pages
• Pages and frames
– Addresses: page number, offset
• Page tables: mapping from page # to frame #
– TLB: page table caching
• Memory protection and sharing
• Multilevel page tables
19
Paging
• Divide physical memory into fixed-sized blocks 
called frames (or page frames)
– Size is power of 2, between 512 bytes and 16 Mbytes
• Divide logical memory into blocks of same size 
called pages
• To run a program of size N pages, need to find N
free frames and load program
• Still have Internal fragmentation
• Physical  address space of a process can be 
noncontiguous; process is allocated physical 
memory whenever the latter is available
– Avoids external fragmentation
– Avoids problem of varying sized memory chunks
20
Address Translation Scheme
• Address generated by CPU is divided into:
– Page number (p) – used as an index into a page 
table which contains base address of each page in 
physical memory
– Page offset (d) – combined with base address to 
define the physical memory address that is sent to 
the memory unit
page number page offset
p d
m -n n
– For given logical address space 2m and page size
2n
22
Paging Hardware
Page number  p  mapped  frame number f.
The offset d needs no mapping.
23
Paging Example
8 frames 
Frame number 0-to-7
Page 0 maps Example: 
to frame 5 Logical add:   00 10 (2)
Phyical Add: 101 10 (22)
Ex: m=4   and  n=2 
• Logical add. space = 24 bytes, 
• 22=4-byte pages
• 32-byte physics memory with 8 frames
25
Paging (Cont.)
• Internal fragmentation
– Ex: Page size = 2,048 bytes, Process size = 72,766 bytes  
• 35 pages + 1,086 bytes
• Internal fragmentation of 2,048 - 1,086 = 962 bytes wasted
– Worst case fragmentation = 1 frame – 1 byte
– On average fragmentation = 1 / 2 frame size
– So small frame sizes desirable?
• But each page table entry takes memory to track
– Page size
• X86-64: 4 KB (common), 2 MB (“huge” for servers), 1GB (“large”)
• Process view and physical memory now very 
different
• By implementation, a process can only access its 
own memory unless ..
26
Free Frame allocation
A new process arrives 
That needs four pages
Before allocation After allocation
27
Implementation of Page Table
Page table is kept in main memory
• Page-table base register (PTBR) points to 
the page table
• Page-table length register (PTLR)
indicates size of the page table One page-tableFor each process
• In this scheme every data/instruction 
access requires two memory accesses
– One for the page table and one for the data / 
instruction
The two memory access problem can be 
solved by the use of a special fast-lookup 
hardware cache called associative memory 
or translation look-aside buffers (TLBs)
TLB: cache for Page Table
28
Caching: The General Concept
• Widely used concept: 
– keep small subset of information likely to 
needed in near future in a fast accessible place
– Hopefully the “Hit Rate” is high
Challenges: 
– 1. Is the information in cache? 2. Where?
– Hit rate vs cache size
Examples: 
– Cache Memory (“Cache”): 
Cache for Main memory  Default meaning for this class
– Browser cache: for browser
– Disk cache
– Cache for Page Table: TLB
29
Implementation of Page Table (Cont.)
• Some TLBs store address-space identifiers 
(ASIDs) in each TLB entry – uniquely identifies 
each process to provide address-space 
protection for that process
– Otherwise need to flush TLB at every context switch
• TLBs typically small (64 to 1,024 entries)
• On a TLB miss, value is loaded into the TLB for 
faster access next time
– Replacement policies must be considered
– Some entries can be wired down for permanent fast 
access TLB: cache for 
page Table
30
Associative Memory
• Associative memory – parallel search using hardware
– “Content addressable memory”: Electronics is very expensive
Page # Frame #
• Address translation (p, d)
– If p is in associative register, get frame # out  (“Hit”)
– Otherwise get frame # from page table in memory  (“Miss”)
31
Paging Hardware With TLB
TLB Miss: page table access may be 
done using hardware or software
32
Effective Access Time
• Associative Lookup = e time units
– Can be < 10% of memory access time (mat)
• Hit ratio = a
– Hit ratio – percentage of times that a page number is 
found in the associative registers; ratio related to 
number of associative registers
• Effective Access Time (EAT): probability weighted
EAT = a (mat + e) + (1 – a)(2.mat+e) 
Ex:
Consider a = 80%, e = negligible for TLB search, 
100ns for memory access time
– EAT = 0.80 x 100 + 0.20 x 200 = 120ns
• Consider more realistic hit ratio ->  a = 99%, 
– EAT = 0.99 x 100 + 0.01 x 200 = 101ns
33
Memory Protection
• Memory protection implemented by associating 
protection bit with each frame to indicate if 
read-only or read-write access is allowed
– Can also add more bits to indicate page execute-
only, and so on
• Valid-invalid bit attached to each entry in the 
page table:
– “valid” indicates that the associated page is in the 
process’ logical address space, and is thus a legal 
page
– “invalid” indicates that the page is not in the 
process’ logical address space
• Any violations result in a trap to the kernel
34
Valid (v) or Invalid (i) Bit In A Page Table
“invalid” : page is not 
in the process’s 
address space. 
35
Shared Pages among Processes
• Shared code
– One copy of read-only (reentrant non-self modifying) 
code shared among processes (i.e., text editors, 
compilers, window systems)
– Similar to multiple threads sharing the same 
process space
– Also useful for interprocess communication if 
sharing of read-write pages is allowed
• Private code and data
– Each process keeps a separate copy of the 
code and data
– The pages for the private code and data can 
appear anywhere in the logical address space
36
Shared Pages Example
ed1, ed2, ed3
(frames 3, 4, 6) shared
37
Overheads in paging:   Page table and internal fragmentation 
Optimal Page Size: 
page table size vs internal  fragmentation tradeoff
• Average process size = s
• Page size = p
• Size of each entry in page table = e 
– Pages per process = s/p 
– se/p: Total page table space for average process
• Total Overhead = Page table overhead + 
Internal fragmentation loss 
= se/p + p/2 
38
Optimal Page size: Page table and internal fragmentation 
• Total Overhead = se/p + p/2 
• Optimal: Obtain derivative of overhead with 
respect to p, equate to 0 
-se/p2 +1⁄2 = 0 
• i.e.     p2 =2se    or p = (2se)0.5
Assume   s = 128KB and e=8 bytes per entry 
• Optimal page size = 1448 bytes
– In practice we will never use 1448 bytes 
– Instead, either 1K or 2K would be used 
• Why? Pages sizes are in powers of 2 i.e. 2X
• Deriving offsets and page numbers is also easier 
39
Page Table Size
Memory structures for paging can get huge using 
straight-forward methods
• Consider a 32-bit logical address space as on 
recent processors 64-bit on 64-bit processors
– Page size of 4 KB (212) entries
– Page table would have 1 million entries (232 / 212)
– If each entry is 4 bytes -> 4 MB of physical address 
space / memory for page table alone
• Don’t want to allocate that contiguously in main memory
210 1024  or 1 kibibyte
220 1M mebibyte
230 1G      gigibyte
240 1T       tebibyte
40
Issues with large page tables 
• Cannot allocate page table contiguously in 
memory   
• Solution: 
– Divide the page table into smaller pieces 
– Page the page-table 
• Hierarchical Paging
41
Hierarchical Page Tables
• Break up the logical address 
space into multiple page tables
• A simple technique is a two-level 
page table
• We then page the page table
P1: indexes the outer page table
P2:  page table: maps to frame
42
Next
43
Two-Level Page-Table Scheme
44
Two-Level Paging Example
• A logical address (on 32-bit machine with 1K page 
size) is divided into:
– a page number consisting of 22 bits
– a page offset consisting of 10 bits
• Since the page table is paged, the page number is 
further divided into:
– a 12-bit page number 
– a 10-bit page offset
• Thus, a logical address is as follows:
• where p1 is an index into the outer page table, and pis the displacement within the page of the inner page2 
table
• Known as forward-mapped page table
45
Two-Level Paging Example
• A logical address is as follows:
• One Outer page table: size 212
• Often only some of all possible 212 Page 
tables needed (each of size 210)
46
